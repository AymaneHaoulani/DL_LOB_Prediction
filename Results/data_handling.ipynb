{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec71fe04-cc4b-442f-820e-0effd8a37b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32512f2-5cb0-465d-9eeb-c0e5c25224a1",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc24e244-89a4-44f7-9645-96cb8199c193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./BTCUSDT/20241014\\bid_ask_depth_10_data_BTCUSDT_20241014_235210.csv\n",
      "./BTCUSDT/20241015\\bid_ask_depth_10_data_BTCUSDT_20241015_235214.csv\n",
      "./BTCUSDT/20241016\\bid_ask_depth_10_data_BTCUSDT_20241016_235218.csv\n",
      "./BTCUSDT/20241017\\bid_ask_depth_10_data_BTCUSDT_20241017_235222.csv\n",
      "./BTCUSDT/20241018\\bid_ask_depth_10_data_BTCUSDT_20241018_235226.csv\n",
      "./BTCUSDT/20241019\\bid_ask_depth_10_data_BTCUSDT_20241019_235230.csv\n",
      "./BTCUSDT/20241020\\bid_ask_depth_10_data_BTCUSDT_20241020_235234.csv\n",
      "./BTCUSDT/20241021\\bid_ask_depth_10_data_BTCUSDT_20241021_235238.csv\n",
      "./BTCUSDT/20241022\\bid_ask_depth_10_data_BTCUSDT_20241022_235242.csv\n",
      "./BTCUSDT/20241023\\bid_ask_depth_10_data_BTCUSDT_20241023_235246.csv\n",
      "./BTCUSDT/20241024\\bid_ask_depth_10_data_BTCUSDT_20241024_235250.csv\n",
      "./BTCUSDT/20241025\\bid_ask_depth_10_data_BTCUSDT_20241025_235254.csv\n",
      "./BTCUSDT/20241026\\bid_ask_depth_10_data_BTCUSDT_20241026_235258.csv\n",
      "./BTCUSDT/20241027\\bid_ask_depth_10_data_BTCUSDT_20241027_235302.csv\n",
      "./BTCUSDT/20241028\\bid_ask_depth_10_data_BTCUSDT_20241028_235306.csv\n",
      "./BTCUSDT/20241029\\bid_ask_depth_10_data_BTCUSDT_20241029_235310.csv\n",
      "./BTCUSDT/20241030\\bid_ask_depth_10_data_BTCUSDT_20241030_235314.csv\n",
      "./BTCUSDT/20241031\\bid_ask_depth_10_data_BTCUSDT_20241031_235318.csv\n",
      "./BTCUSDT/backup\\bid_ask_depth_10_data_BTCUSDT_20241014_235210.csv\n"
     ]
    }
   ],
   "source": [
    "main_folder_path = './BTCUSDT/'\n",
    "\n",
    "csv_files = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(main_folder_path):\n",
    "    # Filtre les fichiers CSV\n",
    "    csv_files_in_subdir = [f for f in files if f.endswith('.csv')]\n",
    "    \n",
    "    if csv_files_in_subdir:\n",
    "        # Trie les fichiers CSV par ordre alphabétique et prend le dernier\n",
    "        csv_files_in_subdir.sort()\n",
    "        last_csv = csv_files_in_subdir[-1]\n",
    "        csv_files.append(os.path.join(subdir, last_csv))\n",
    "\n",
    "# Affiche la liste des chemins des derniers fichiers CSV par sous-dossier\n",
    "for csv_file in csv_files:\n",
    "    print(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbf7650b-2edb-4b1a-8dc5-2c8f68e9a33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Update ID</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Bid Price 1</th>\n",
       "      <th>Bid Volume 1</th>\n",
       "      <th>Ask Price 1</th>\n",
       "      <th>Ask Volume 1</th>\n",
       "      <th>Bid Price 2</th>\n",
       "      <th>Bid Volume 2</th>\n",
       "      <th>Ask Price 2</th>\n",
       "      <th>Ask Volume 2</th>\n",
       "      <th>...</th>\n",
       "      <th>Ask Price 8</th>\n",
       "      <th>Ask Volume 8</th>\n",
       "      <th>Bid Price 9</th>\n",
       "      <th>Bid Volume 9</th>\n",
       "      <th>Ask Price 9</th>\n",
       "      <th>Ask Volume 9</th>\n",
       "      <th>Bid Price 10</th>\n",
       "      <th>Bid Volume 10</th>\n",
       "      <th>Ask Price 10</th>\n",
       "      <th>Ask Volume 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52924983425</td>\n",
       "      <td>2024-10-14 23:52:10.442473+00:00</td>\n",
       "      <td>66103.02</td>\n",
       "      <td>1.98328</td>\n",
       "      <td>66103.03</td>\n",
       "      <td>4.68219</td>\n",
       "      <td>66103.0</td>\n",
       "      <td>0.00406</td>\n",
       "      <td>66103.04</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>...</td>\n",
       "      <td>66103.5</td>\n",
       "      <td>0.00018</td>\n",
       "      <td>66102.01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>66103.73</td>\n",
       "      <td>0.34294</td>\n",
       "      <td>66102.0</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>66103.96</td>\n",
       "      <td>0.02846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52924983430</td>\n",
       "      <td>2024-10-14 23:52:10.542827+00:00</td>\n",
       "      <td>66103.02</td>\n",
       "      <td>1.98134</td>\n",
       "      <td>66103.03</td>\n",
       "      <td>4.68219</td>\n",
       "      <td>66103.0</td>\n",
       "      <td>0.00406</td>\n",
       "      <td>66103.04</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>...</td>\n",
       "      <td>66103.5</td>\n",
       "      <td>0.00018</td>\n",
       "      <td>66102.01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>66103.73</td>\n",
       "      <td>0.34294</td>\n",
       "      <td>66102.0</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>66103.96</td>\n",
       "      <td>0.02846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52924983433</td>\n",
       "      <td>2024-10-14 23:52:10.642524+00:00</td>\n",
       "      <td>66103.02</td>\n",
       "      <td>1.98315</td>\n",
       "      <td>66103.03</td>\n",
       "      <td>4.68219</td>\n",
       "      <td>66103.0</td>\n",
       "      <td>0.00406</td>\n",
       "      <td>66103.04</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>...</td>\n",
       "      <td>66103.5</td>\n",
       "      <td>0.00018</td>\n",
       "      <td>66102.01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>66103.73</td>\n",
       "      <td>0.34294</td>\n",
       "      <td>66102.0</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>66103.96</td>\n",
       "      <td>0.02846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52924983439</td>\n",
       "      <td>2024-10-14 23:52:10.742283+00:00</td>\n",
       "      <td>66103.02</td>\n",
       "      <td>1.98315</td>\n",
       "      <td>66103.03</td>\n",
       "      <td>4.68172</td>\n",
       "      <td>66103.0</td>\n",
       "      <td>0.00406</td>\n",
       "      <td>66103.04</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>...</td>\n",
       "      <td>66103.5</td>\n",
       "      <td>0.00018</td>\n",
       "      <td>66102.01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>66103.73</td>\n",
       "      <td>0.34294</td>\n",
       "      <td>66102.0</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>66103.96</td>\n",
       "      <td>0.02846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52924983441</td>\n",
       "      <td>2024-10-14 23:52:10.842547+00:00</td>\n",
       "      <td>66103.02</td>\n",
       "      <td>1.98315</td>\n",
       "      <td>66103.03</td>\n",
       "      <td>4.68172</td>\n",
       "      <td>66103.0</td>\n",
       "      <td>0.00406</td>\n",
       "      <td>66103.04</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>...</td>\n",
       "      <td>66103.5</td>\n",
       "      <td>0.00018</td>\n",
       "      <td>66102.01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>66103.73</td>\n",
       "      <td>0.34294</td>\n",
       "      <td>66102.0</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>66103.96</td>\n",
       "      <td>0.02846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Update ID                         Timestamp  Bid Price 1  Bid Volume 1  \\\n",
       "0  52924983425  2024-10-14 23:52:10.442473+00:00     66103.02       1.98328   \n",
       "1  52924983430  2024-10-14 23:52:10.542827+00:00     66103.02       1.98134   \n",
       "2  52924983433  2024-10-14 23:52:10.642524+00:00     66103.02       1.98315   \n",
       "3  52924983439  2024-10-14 23:52:10.742283+00:00     66103.02       1.98315   \n",
       "4  52924983441  2024-10-14 23:52:10.842547+00:00     66103.02       1.98315   \n",
       "\n",
       "   Ask Price 1  Ask Volume 1  Bid Price 2  Bid Volume 2  Ask Price 2  \\\n",
       "0     66103.03       4.68219      66103.0       0.00406     66103.04   \n",
       "1     66103.03       4.68219      66103.0       0.00406     66103.04   \n",
       "2     66103.03       4.68219      66103.0       0.00406     66103.04   \n",
       "3     66103.03       4.68172      66103.0       0.00406     66103.04   \n",
       "4     66103.03       4.68172      66103.0       0.00406     66103.04   \n",
       "\n",
       "   Ask Volume 2  ...  Ask Price 8  Ask Volume 8  Bid Price 9  Bid Volume 9  \\\n",
       "0       0.00204  ...      66103.5       0.00018     66102.01        0.0001   \n",
       "1       0.00204  ...      66103.5       0.00018     66102.01        0.0001   \n",
       "2       0.00204  ...      66103.5       0.00018     66102.01        0.0001   \n",
       "3       0.00204  ...      66103.5       0.00018     66102.01        0.0001   \n",
       "4       0.00204  ...      66103.5       0.00018     66102.01        0.0001   \n",
       "\n",
       "   Ask Price 9  Ask Volume 9  Bid Price 10  Bid Volume 10  Ask Price 10  \\\n",
       "0     66103.73       0.34294       66102.0         0.0873      66103.96   \n",
       "1     66103.73       0.34294       66102.0         0.0873      66103.96   \n",
       "2     66103.73       0.34294       66102.0         0.0873      66103.96   \n",
       "3     66103.73       0.34294       66102.0         0.0873      66103.96   \n",
       "4     66103.73       0.34294       66102.0         0.0873      66103.96   \n",
       "\n",
       "   Ask Volume 10  \n",
       "0        0.02846  \n",
       "1        0.02846  \n",
       "2        0.02846  \n",
       "3        0.02846  \n",
       "4        0.02846  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = './BTCUSDT/20241014/bid_ask_depth_10_data_BTCUSDT_20241014_235210.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9752a88d-5c97-4b61-951f-1d221d2e4fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4590\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a83ef8-37ef-4b08-bb0e-0d84fec5fc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5968\n"
     ]
    }
   ],
   "source": [
    "file_path_2 = './BTCUSDT/20241015/bid_ask_depth_10_data_BTCUSDT_20241015_000210.csv'\n",
    "df_2 = pd.read_csv(file_path_2)\n",
    "print(len(df_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06884298-0789-42ba-a208-6e6f75f82065",
   "metadata": {},
   "source": [
    "Two first comments about Bitcoin CSV files for prediction:\n",
    "\n",
    "1. We have 42 columns consisting of:\n",
    "   - **Update ID** (int64)\n",
    "   - **Timestamp** (datetime)\n",
    "   - 10 sets of (Bid Price, Bid Volume, Ask Price, Ask Volume) (float64)\n",
    "\n",
    "2. Each CSV file contains approximately **10 minutes** of data, even though the number of rows is not fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f445c-50e0-416d-a4ab-84047367f1ce",
   "metadata": {},
   "source": [
    "### Let's try to analyse the update of the Time series "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b7fc7-8d67-4b33-81ea-d4c5ae6394a2",
   "metadata": {},
   "source": [
    "At first sight, the updates seem to occur  typically within less than 1 second, and (often as frequently as every 0.1 seconds or even less). Let's try to verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aeccfc4-dc8e-48ca-975e-eeb793168736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [03:35<00:00, 11.32s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.78it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files with timestamp differences greater than 1 second:\n",
      "File: ./BTCUSDT/20241018\\.ipynb_checkpoints\\bid_ask_depth_10_data_BTCUSDT_20241018_073223-checkpoint.csv, Timestamp Difference: 0 days 00:00:32.281585\n",
      "File: ./BTCUSDT/20241018\\bid_ask_depth_10_data_BTCUSDT_20241018_073223.csv, Timestamp Difference: 0 days 00:00:32.281585\n",
      "File: ./BTCUSDT/20241018\\bid_ask_depth_10_data_BTCUSDT_20241018_224225.csv, Timestamp Difference: 0 days 00:00:01.720473\n",
      "File: ./BTCUSDT/20241019\\bid_ask_depth_10_data_BTCUSDT_20241019_002226.csv, Timestamp Difference: 0 days 00:00:16.133100\n",
      "File: ./BTCUSDT/20241024\\bid_ask_depth_10_data_BTCUSDT_20241024_051247.csv, Timestamp Difference: 0 days 00:00:58.970320\n",
      "File: ./BTCUSDT/20241024\\bid_ask_depth_10_data_BTCUSDT_20241024_052247.csv, Timestamp Difference: 0 days 00:07:25.403427\n",
      "File: ./BTCUSDT/20241024\\bid_ask_depth_10_data_BTCUSDT_20241024_053247.csv, Timestamp Difference: 0 days 00:04:29.527132\n",
      "File: ./BTCUSDT/20241024\\bid_ask_depth_10_data_BTCUSDT_20241024_055247.csv, Timestamp Difference: 0 days 00:01:05.791378\n",
      "File: ./BTCUSDT/20241024\\bid_ask_depth_10_data_BTCUSDT_20241024_070247.csv, Timestamp Difference: 0 days 00:04:24.586937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from helpers import file_check\n",
    "\n",
    "main_folder_path = './BTCUSDT/'\n",
    "\n",
    "# Call the function and get the list of files with timestamp differences\n",
    "result = file_check.find_timestamp_diff(main_folder_path)\n",
    "\n",
    "result = sorted(result, key=lambda x: x[0])\n",
    "\n",
    "# Print the result\n",
    "print(\"Files with timestamp differences greater than 1 second:\")\n",
    "for file_path, diff in result:\n",
    "    print(f\"File: {file_path}, Timestamp Difference: {diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd31777-def1-40cc-8f74-6fdbdb032f51",
   "metadata": {},
   "source": [
    "We found 8 instances where the timestamp gaps between consecutive files exceeded one second, ranging from 1 second to 7 minutes. The most likely reasons for these gaps include periods of low market activity, data transmission delays, exchange maintenance, API rate limiting, or potential market events causing irregularities in data reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1712f0a-84e5-4316-af3c-aba835b98463",
   "metadata": {},
   "source": [
    "## Action Plan for the Next Steps in the Notebook\n",
    "\n",
    "1. **Verify Data Completeness**  \n",
    "   Ensure the data is complete, with no missing values or inconsistencies.\n",
    "\n",
    "2. **Clean Entries**  \n",
    "   Move entries that fall outside the desired date range.\n",
    "   For the first case it will be:\n",
    "   - Exclude data from **14 October** (before midnight).  \n",
    "   - Exclude data from the start of **1 November**.\n",
    "   - Separate CSV files that contain data spanning across two different days.\n",
    "\n",
    "3. **Create Larger Datasets**  \n",
    "   Combine smaller datasets to create larger ones, such as daily datasets.\n",
    "\n",
    "4. **Label the Data**  \n",
    "   Introduce labels for the data based on trends:  \n",
    "   - **up**, **down** and **stable**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6c4521-020e-484c-8538-fa236493dd05",
   "metadata": {},
   "source": [
    "### 1. Verify Data Completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e2a99-3128-4611-bb40-16693167464b",
   "metadata": {},
   "source": [
    "The first step is to ensure the data is complete, so we'll begin by checking for any missing values and verifying that all price and volume columns contain positive values. If any discrepancies are found, it could indicate potential issues with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "531b412c-7729-460c-8706-3f208fcbf7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [01:35<00:00,  5.04s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.30it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.70it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.21it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing or invalid values found in any files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from helpers import wrong_values_check\n",
    "\n",
    "main_folder_path = './BTCUSDT/'\n",
    "\n",
    "# Call the function to check for missing and invalid values\n",
    "result = wrong_values_check.check_missing_and_invalid_values(main_folder_path)\n",
    "\n",
    "# Print the result\n",
    "if result:\n",
    "    print(\"Files with missing or invalid values:\")\n",
    "    for file_path, issues in result:\n",
    "        print(f\"File: {file_path}\")\n",
    "        if 'missing' in issues:\n",
    "            print(f\"  Missing Columns: {issues['missing']}\")\n",
    "        if 'invalid' in issues:\n",
    "            print(f\"  Invalid Columns: {issues['invalid']}\")\n",
    "else:\n",
    "    print(\"No missing or invalid values found in any files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf10c4-2762-4d44-b41f-59fc3bd5a791",
   "metadata": {},
   "source": [
    "Currently, there are no missing values or anomalies in the price and volume columns. A more in-depth analysis, such as verifying unique IDs and ensuring the time series are properly ordered, would be useful to further confirm the integrity of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b715f-1c05-4629-9327-63f331391e74",
   "metadata": {},
   "source": [
    "### 2.Clean Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce515f78-190d-48f5-ae96-afbbaaa6a933",
   "metadata": {},
   "source": [
    "To ensure accurate analysis within a specified time range, we need to address potential overlaps in the dataset. For example, CSV files might contain data from the previous day or extend into the following day, with entries that overlap between the two periods.  To correct this, we will trim the CSV files by removing sorting entries by correct date foleder to ensure in the future a clean analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "496c7736-784d-4015-a16c-16e9ad973db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2024, 10, 14)]\n"
     ]
    }
   ],
   "source": [
    "file_path_14 = './BTCUSDT/20241014/bid_ask_depth_10_data_BTCUSDT_20241014_235210.csv'\n",
    "df = pd.read_csv(file_path_14)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "unique_days = df['Timestamp'].dt.date.unique()\n",
    "print(unique_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8a1e960-0a51-4be8-ad8f-028a8cbd23c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2024, 10, 31) datetime.date(2024, 11, 1)]\n"
     ]
    }
   ],
   "source": [
    "file_path_31 = './BTCUSDT/20241031/bid_ask_depth_10_data_BTCUSDT_20241031_235318.csv'\n",
    "df = pd.read_csv(file_path_31)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "unique_days = df['Timestamp'].dt.date.unique()\n",
    "print(unique_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc604ba-3c28-4ac0-b5bf-91ec1f3ffce8",
   "metadata": {},
   "source": [
    "To allow for a clearer separation, CSV files with overlapping days are split into two: one retains the same name and marks the end of the current day, while the other, representing the start of the following day, is moved to the folder of the next day. The goal is to ensure that the folders are clearly organized and separated, making it easier to proceed with the next step.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a7709bb-1a1b-4d4c-9e6d-a7a4d994fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./BTCUSDT/20241014\\bid_ask_depth_10_data_BTCUSDT_20241014_235210.csv\n",
      "The file ./BTCUSDT/20241014\\bid_ask_depth_10_data_BTCUSDT_20241014_235210.csv has been saved.\n",
      "./BTCUSDT/20241014\n",
      "The file ./BTCUSDT/20241014\\bid_ask_depth_10_data_BTCUSDT_20241014_235210.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241015\\bid_ask_depth_10_data_BTCUSDT_20241015_235214.csv\n",
      "The file ./BTCUSDT/20241015\\bid_ask_depth_10_data_BTCUSDT_20241015_235214.csv has been saved.\n",
      "./BTCUSDT/20241015\n",
      "The file ./BTCUSDT/20241015\\bid_ask_depth_10_data_BTCUSDT_20241015_235214.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241016\\bid_ask_depth_10_data_BTCUSDT_20241016_235218.csv\n",
      "The file ./BTCUSDT/20241016\\bid_ask_depth_10_data_BTCUSDT_20241016_235218.csv has been saved.\n",
      "./BTCUSDT/20241016\n",
      "The file ./BTCUSDT/20241016\\bid_ask_depth_10_data_BTCUSDT_20241016_235218.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241017\\bid_ask_depth_10_data_BTCUSDT_20241017_235222.csv\n",
      "The file ./BTCUSDT/20241017\\bid_ask_depth_10_data_BTCUSDT_20241017_235222.csv has been saved.\n",
      "./BTCUSDT/20241017\n",
      "The file ./BTCUSDT/20241017\\bid_ask_depth_10_data_BTCUSDT_20241017_235222.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241018\\bid_ask_depth_10_data_BTCUSDT_20241018_235226.csv\n",
      "The file ./BTCUSDT/20241018\\bid_ask_depth_10_data_BTCUSDT_20241018_235226.csv has been saved.\n",
      "./BTCUSDT/20241018\n",
      "The file ./BTCUSDT/20241018\\bid_ask_depth_10_data_BTCUSDT_20241018_235226.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241019\\bid_ask_depth_10_data_BTCUSDT_20241019_235230.csv\n",
      "The file ./BTCUSDT/20241019\\bid_ask_depth_10_data_BTCUSDT_20241019_235230.csv has been saved.\n",
      "./BTCUSDT/20241019\n",
      "The file ./BTCUSDT/20241019\\bid_ask_depth_10_data_BTCUSDT_20241019_235230.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241020\\bid_ask_depth_10_data_BTCUSDT_20241020_235234.csv\n",
      "The file ./BTCUSDT/20241020\\bid_ask_depth_10_data_BTCUSDT_20241020_235234.csv has been saved.\n",
      "./BTCUSDT/20241020\n",
      "The file ./BTCUSDT/20241020\\bid_ask_depth_10_data_BTCUSDT_20241020_235234.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241021\\bid_ask_depth_10_data_BTCUSDT_20241021_235238.csv\n",
      "The file ./BTCUSDT/20241021\\bid_ask_depth_10_data_BTCUSDT_20241021_235238.csv has been saved.\n",
      "./BTCUSDT/20241021\n",
      "The file ./BTCUSDT/20241021\\bid_ask_depth_10_data_BTCUSDT_20241021_235238.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241022\\bid_ask_depth_10_data_BTCUSDT_20241022_235242.csv\n",
      "The file ./BTCUSDT/20241022\\bid_ask_depth_10_data_BTCUSDT_20241022_235242.csv has been saved.\n",
      "./BTCUSDT/20241022\n",
      "The file ./BTCUSDT/20241022\\bid_ask_depth_10_data_BTCUSDT_20241022_235242.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241023\\bid_ask_depth_10_data_BTCUSDT_20241023_235246.csv\n",
      "The file ./BTCUSDT/20241023\\bid_ask_depth_10_data_BTCUSDT_20241023_235246.csv has been saved.\n",
      "./BTCUSDT/20241023\n",
      "The file ./BTCUSDT/20241023\\bid_ask_depth_10_data_BTCUSDT_20241023_235246.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241024\\bid_ask_depth_10_data_BTCUSDT_20241024_235250.csv\n",
      "The file ./BTCUSDT/20241024\\bid_ask_depth_10_data_BTCUSDT_20241024_235250.csv has been saved.\n",
      "./BTCUSDT/20241024\n",
      "The file ./BTCUSDT/20241024\\bid_ask_depth_10_data_BTCUSDT_20241024_235250.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241025\\bid_ask_depth_10_data_BTCUSDT_20241025_235254.csv\n",
      "The file ./BTCUSDT/20241025\\bid_ask_depth_10_data_BTCUSDT_20241025_235254.csv has been saved.\n",
      "./BTCUSDT/20241025\n",
      "The file ./BTCUSDT/20241025\\bid_ask_depth_10_data_BTCUSDT_20241025_235254.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241026\\bid_ask_depth_10_data_BTCUSDT_20241026_235258.csv\n",
      "The file ./BTCUSDT/20241026\\bid_ask_depth_10_data_BTCUSDT_20241026_235258.csv has been saved.\n",
      "./BTCUSDT/20241026\n",
      "The file ./BTCUSDT/20241026\\bid_ask_depth_10_data_BTCUSDT_20241026_235258.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241027\\bid_ask_depth_10_data_BTCUSDT_20241027_235302.csv\n",
      "The file ./BTCUSDT/20241027\\bid_ask_depth_10_data_BTCUSDT_20241027_235302.csv has been saved.\n",
      "./BTCUSDT/20241027\n",
      "The file ./BTCUSDT/20241027\\bid_ask_depth_10_data_BTCUSDT_20241027_235302.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241028\\bid_ask_depth_10_data_BTCUSDT_20241028_235306.csv\n",
      "The file ./BTCUSDT/20241028\\bid_ask_depth_10_data_BTCUSDT_20241028_235306.csv has been saved.\n",
      "./BTCUSDT/20241028\n",
      "The file ./BTCUSDT/20241028\\bid_ask_depth_10_data_BTCUSDT_20241028_235306.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241029\\bid_ask_depth_10_data_BTCUSDT_20241029_235310.csv\n",
      "The file ./BTCUSDT/20241029\\bid_ask_depth_10_data_BTCUSDT_20241029_235310.csv has been saved.\n",
      "./BTCUSDT/20241029\n",
      "The file ./BTCUSDT/20241029\\bid_ask_depth_10_data_BTCUSDT_20241029_235310.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241030\\bid_ask_depth_10_data_BTCUSDT_20241030_235314.csv\n",
      "The file ./BTCUSDT/20241030\\bid_ask_depth_10_data_BTCUSDT_20241030_235314.csv has been saved.\n",
      "./BTCUSDT/20241030\n",
      "The file ./BTCUSDT/20241030\\bid_ask_depth_10_data_BTCUSDT_20241030_235314.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241031\\bid_ask_depth_10_data_BTCUSDT_20241031_235318.csv\n",
      "The file ./BTCUSDT/20241031\\bid_ask_depth_10_data_BTCUSDT_20241031_235318.csv has been saved.\n",
      "./BTCUSDT/20241031\n",
      "The file ./BTCUSDT/20241031\\bid_ask_depth_10_data_BTCUSDT_20241031_235318.csv has been saved.\n",
      "Processing file: ./BTCUSDT/20241101\\bid_ask_depth_10_data_BTCUSDT_20241101_000000.csv\n",
      "The file ./BTCUSDT/20241101\\bid_ask_depth_10_data_BTCUSDT_20241101_000000.csv has been saved.\n",
      "./BTCUSDT/20241101\n",
      "The file ./BTCUSDT/20241101\\bid_ask_depth_10_data_BTCUSDT_20241101_000000.csv has been saved.\n"
     ]
    }
   ],
   "source": [
    "from helpers import split_days\n",
    "\n",
    "file_path = './BTCUSDT/'\n",
    "split_days.process_last_files_in_directory(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08abb0e5-b419-4ebd-adf0-cd774b7a14c1",
   "metadata": {},
   "source": [
    "### 3.Create Larger Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b451533-ad49-4f0b-9d4c-af074336485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import convert_data_to_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250ff7bf-ea73-4f98-9f24-2e2996b5b4c6",
   "metadata": {},
   "source": [
    "Now that we have folders perfectly organized by day with their respective CSV files, it may be useful to merge them into a single large dataset for each day !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a3b80d4-f133-4776-9716-4c941f4654c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  95%|██████████████████████████████████████████████████████████▉   | 19/20 [10:51<00:34, 34.30s/date]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_directory, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Process and save data\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m convert_data_to_daily\u001b[38;5;241m.\u001b[39mprocess_and_save_data(main_folder_path, output_directory, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBTCUSDT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive\\Bureau\\ML_WORK\\helpers\\convert_data_to_daily.py:55\u001b[0m, in \u001b[0;36mprocess_and_save_data\u001b[1;34m(main_folder_path, output_directory, symbol)\u001b[0m\n\u001b[0;32m     52\u001b[0m next_subdir \u001b[38;5;241m=\u001b[39m subdirs_for_date[idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(subdirs_for_date) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Process the current subdirectory and adjacent subdirectories\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m accumulated_data \u001b[38;5;241m=\u001b[39m process_subdirectory(subdirs_for_date[\u001b[38;5;241m0\u001b[39m], date)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_subdir:\n\u001b[0;32m     57\u001b[0m     prev_data \u001b[38;5;241m=\u001b[39m process_subdirectory(prev_subdir, date)\n",
      "File \u001b[1;32m~\\OneDrive\\Bureau\\ML_WORK\\helpers\\convert_data_to_daily.py:29\u001b[0m, in \u001b[0;36mprocess_subdirectory\u001b[1;34m(subdir_path, date)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m selected_data\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     27\u001b[0m         dfs\u001b[38;5;241m.\u001b[39mappend(selected_data)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    385\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[0;32m    386\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[0;32m    387\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[0;32m    388\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[0;32m    389\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[0;32m    390\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[0;32m    391\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# orderbook\n",
    "main_folder_path = './BTCUSDT/'\n",
    "output_directory = './BTCUSDT' + 'daily/'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process and save data\n",
    "convert_data_to_daily.process_and_save_data(main_folder_path, output_directory, 'BTCUSDT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a73ff-4187-4c56-93c7-7dc37dec8a99",
   "metadata": {},
   "source": [
    "Now that we have one CSV per day containing all the data from 00:00 to 23:59, the next step will be to label the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e876bf24-c1a0-4989-a0f5-7ac225e3f75a",
   "metadata": {},
   "source": [
    "### 4.Label the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43fb89-63de-4663-96bf-f3d0dc12e7e9",
   "metadata": {},
   "source": [
    "To begin this step, we will focus on a specific day, October 15, and apply our various labeling ideas to the dataset corresponding to this day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef78f17-2942-4ae4-b556-1ec074dc708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_15october = './BTCUSDTdaily/bid_ask_data_BTCUSDT_20241015.csv'\n",
    "df = pd.read_csv(file_path_15october)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69640e5d-3492-4f5e-a6d2-350fae12189d",
   "metadata": {},
   "source": [
    "#### 4.1 Naive Labelling  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a02eed-ba82-4132-aeec-6281b7d7fb8d",
   "metadata": {},
   "source": [
    "**Definition of mid price** \n",
    "The mid price is calculated as the average of the best ask price and the best bid price from the order book. In our case it's Ask and Bid Price 1\n",
    "\n",
    "The formula is given by:  \n",
    "\n",
    "$$m(t) = \\frac{P_{\\text{ask}}(t) + P_{\\text{bid}}(t)}{2}$$\n",
    "\n",
    "For each row in the dataset, we observe the variation in mid price compared to the kth previous row and assign a label based on the following rules:  \n",
    "\n",
    "1. **Stable (Label = 0)**: If the mid price remains unchanged between consecutive rows.  \n",
    "2. **Up (Label = 1)**: If the mid price increases compared to the previous row.  \n",
    "3. **Down (Label = -1)**: If the mid price decreases compared to the previous row.  \n",
    "\n",
    "The algorithm can be described as follows:  \n",
    "\n",
    "\n",
    "\\begin{cases} \n",
    "-1 & \\text{if } \\Delta_{\\text{m}} < 0,\\\\\n",
    "0 & \\text{if } \\Delta_{\\text{m}} = 0, \\\\\n",
    "1 & \\text{if } \\Delta_{\\text{m}} > 0 \n",
    "\\end{cases}\n",
    "\n",
    "\n",
    "Where:  \n",
    "\n",
    "$$\\Delta_{\\text{m}} = \\text{m(t+k)} - \\text{m(t)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa512a00-c653-46a6-bde2-ac0b26ce04d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts: {0: {'name': 'Stable', 'count': 582411, 'percentage': 67.81533710828475}, 1: {'name': 'Increasing', 'count': 138246, 'percentage': 16.097221882608558}, -1: {'name': 'Decreasing', 'count': 138162, 'percentage': 16.087441009106694}}\n"
     ]
    }
   ],
   "source": [
    "from helpers import notes \n",
    "\n",
    "df = notes.add_mid_price_label(df,k=10)\n",
    "label_counts = notes.count_labels(df)\n",
    "\n",
    "print(\"Label counts:\", label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad66d4-8ccf-46c4-a5a7-706412ba0907",
   "metadata": {},
   "source": [
    "There are some limitations : \n",
    "\n",
    "- **Noise Sensitivity:** Small fluctuations in prices may lead to mislabeling.  \n",
    "- **Lack of Context:** The algorithm does not consider broader trends or global variations.\n",
    "- **Unbalanced Dataset** In our case stable is represented as 92,3 % which is too much"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f9d25f-671a-4930-82a0-60f72dbe4fc3",
   "metadata": {},
   "source": [
    "#### 4.2 Average of the mid-prices over a future horizon "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b85c3f6-e48c-4bdc-94a7-09e3d04a4e58",
   "metadata": {},
   "source": [
    "To classify the trend, a direct comparison of consecutive mid-prices can be noisy due to market fluctuations. Instead, we use the average of the mid-prices over a future horizon \\( k \\), defined as:\n",
    "\n",
    "$$\\overline{m}(k, t) = \\frac{1}{k} \\sum_{i=1}^{k} m(t + i)$$\n",
    "\n",
    "Where:\n",
    "- $k$ is the horizon (the number of future time units considered), we predict the trend between time $t$ and $t+k$.\n",
    "\n",
    "The trend at time \\( t \\) is determined by comparing the average mid-price  $\\overline{m}(k, t)$ with the current mid-price $m(t)$ and applying a static threshold $\\theta \\in (0, 1)$:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\text{U} & \\text{if } \\overline{m}(k, t) > m(t) \\times (1 + \\theta) \\\\\n",
    "\\text{D} & \\text{if } \\overline{m}(k, t) < m(t) \\times (1 - \\theta) \\\\\n",
    "\\text{S} & \\text{if } \\overline{m}(k, t) \\in [m(t) \\times (1 - \\theta), m(t) \\times (1 + \\theta)] \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This labelling approach smooths out price fluctuations and helps to avoid overfitting by considering a longer-term view of price trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cbecc956-ca49-41af-be44-bbe6fcae11b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(file_path_15october)\n",
    "data['Midprice'] = (data['Bid Price 1'] + data['Ask Price 1']) / 2\n",
    "\n",
    "def compute_encoded_midprice_variation(data, horizon, theta):\n",
    "\n",
    "    future_means = data['Midprice'].shift(-1).rolling(window=horizon, min_periods=1).mean()\n",
    "    variation = (future_means - data['Midprice']) / data['Midprice'] * 100\n",
    "    data['label'] = pd.cut(variation, bins=[-np.inf, -theta, theta, np.inf], labels=[-1, 0, 1])\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "df = compute_encoded_midprice_variation(data,100,0.01)\n",
    "label_counts = notes.count_labels(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "afc342df-6548-4ec5-af8b-433435f63428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts: {0: {'name': 'Stable', 'count': 567535, 'percentage': 66.08319098669219}, 1: {'name': 'Increasing', 'count': 142208, 'percentage': 16.558553082779955}, 2: {'name': 'Decreasing', 'count': 149076, 'percentage': 17.358255930527854}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Label counts:\", label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366afa2-a8be-4297-8569-e76c05e54d7c",
   "metadata": {},
   "source": [
    "The dataset is still not balanced, let's try to find a way to find best parameters in order to resolve this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa35096-5920-45e2-b8f3-02f5745a04a9",
   "metadata": {},
   "source": [
    "**Entropy Function**\n",
    "\n",
    "We will use the entropy fonction, to determine if a distribution is balanced or not.\n",
    "\n",
    "The normalized entropy for a distribution over 3 classes is calculated as:\n",
    "\n",
    "$$\n",
    "H = -\\frac{1}{\\log(3)} \\sum_{i=1}^{3} p_i \\log(p_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $p_i$ is proportion of the $i^{th}$ class.\n",
    "- $log(3)$ is the maximum possible entropy for 3 classes, used for normalization.\n",
    "- $H$ is the normalized entropy, which lies between 0 (totaly unbalanced) and 1 (perfectly balanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3f61193d-4443-46f3-8bd7-4ab29006f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_entropy(df):\n",
    "\n",
    "    class_counts = df[\"label\"].value_counts(normalize = True)\n",
    "    entropy = sum([- p * np.log(p) for p in class_counts if p > 0]) / np.log(3)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3d81507f-7da6-4d20-a442-d6b27a7351f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Entropy: 0.7518904169581898\n",
      "Optimal Parameters: Horizon = 10, Theta = 1e-06\n",
      "Maximum Entropy: 0.9449324308258623\n",
      "Optimal Parameters: Horizon = 25, Theta = 1e-06\n",
      "Maximum Entropy: 0.9998919648296261\n",
      "Optimal Parameters: Horizon = 50, Theta = 1e-06\n",
      "Maximum Entropy: 0.9998675033948783\n",
      "Optimal Parameters: Horizon = 100, Theta = 0.0019200000000000003\n"
     ]
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_entropy_vs_parameters(data, horizon, thetas):\n",
    "    results = []\n",
    "    max_entropy = 0\n",
    "    optimal_theta = None\n",
    "    \n",
    "    for theta in thetas:\n",
    "        # Compute entropy for the current pair of parameters\n",
    "        labels_column = compute_encoded_midprice_variation(data, horizon, theta)\n",
    "        entropy = get_distribution_entropy(labels_column)\n",
    "        \n",
    "        # Store the results\n",
    "        results.append((horizon, theta, entropy))\n",
    "        \n",
    "        # Track the maximum entropy and corresponding parameters\n",
    "        if entropy > max_entropy:\n",
    "            max_entropy = entropy\n",
    "            optimal_theta = theta\n",
    "    \n",
    "    return results, max_entropy, optimal_theta\n",
    "\n",
    "thetas = np.linspace(0.000001, 0.01, 100)  # Example range of thetas\n",
    "\n",
    "#Let's compute the optimal theta for various fixed horizons\n",
    "horizons = [10,25,50,100] \n",
    "\n",
    "for horizon in horizons:\n",
    "    # Compute entropy and find the optimal parameters\n",
    "    results, max_entropy, optimal_theta = compute_entropy_vs_parameters(data, horizon, thetas)\n",
    "    print(f\"Maximum Entropy: {max_entropy}\")\n",
    "    print(f\"Optimal Parameters: Horizon = {horizon}, Theta = {optimal_theta}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310ca17-6ec5-4d44-9c52-6699dec52671",
   "metadata": {},
   "source": [
    "By using this approach, the dataset is balanced by selecting an appropriate value of theta for a fixed horizon, which helps to classify the trends effectively. However, this technique can potentially lead to overfitting, especially when a large portion of the data is classified as stable, thereby skewing the dataset balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f042d-b4f6-4cd3-a673-1abc13e1ebf2",
   "metadata": {},
   "source": [
    "However, a completely balanced dataset may be detrimental, as the model could learn to predict upward and downward trends more than what it is supposed to capture(there can still be noise). It might be more beneficial to increase the number of stable labels compared to the up and down labels to mitigate this issue. The problem here is that the model might not see certain upward or downward movements. Here is the trade off. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
